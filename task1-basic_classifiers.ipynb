{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7OSce2LHK_B"
   },
   "source": [
    "# Assignment 1, Task 1 - Basic ML Classifiers\n",
    "\n",
    "In this task, you are going to implement two classifers and apply them to the  CIFAR-10 dataset: \n",
    "\n",
    "(1) Logistic regression classifier, and\n",
    "\n",
    "(2) Softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mLE5oHAkHK_C"
   },
   "outputs": [],
   "source": [
    "# Import modules, make sure you have installed all required packages before you start.\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.cifar_utils import load_data\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMZSC2GLHK_F"
   },
   "source": [
    "## Load CIFAR-10 data\n",
    "\n",
    "CIFAR-10 is a widely used dataset which contains 60,000 color images of size 32x32 divided into 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. We are going to use them to create our training set, validation set and test set.\n",
    "\n",
    "See https://www.cs.toronto.edu/~kriz/cifar.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-class dataset\n",
    "First, we load the raw CIFAR-10 data and create a 10-class dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7274,
     "status": "ok",
     "timestamp": 1559884655914,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "bV0hpeEqHK_G",
    "outputId": "c4b54f60-33c3-4d00-c783-316c1de30020",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data. You can learn how to use CIFAR dataset by studing cifar_utils code,\n",
    "# but you don't need to worry about that when doing this task.\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "# We have vectorized the data (rearranged the storage of images) for you. \n",
    "# That is, we flattened 1×32×32×3 images into 1×3072 Numpy arrays. Number 3 stands for 3 color channels.\n",
    "# The reason we do this is because we can not put 3-D image representations into our model. \n",
    "# This is common practice (flattening images before putting them into the ML model). \n",
    "# Note that this practice may not be used for Convolutional Neural Networks (CNN). \n",
    "# We will later see how we manage the data when used in CNNs in later assignments.\n",
    "\n",
    "# Check the results\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8435,
     "status": "ok",
     "timestamp": 1559884658097,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "6n4v89BcHK_L",
    "outputId": "9141d16e-b67a-4d60-a7ed-0eb350bc4748"
   },
   "outputs": [],
   "source": [
    "# This part will walk you through how to visualize the CIFAR-10 training dataset.\n",
    "# We first find names of 10 categories.\n",
    "f = open('data/cifar-10-batches-py/batches.meta', 'rb')\n",
    "namedict = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "category = namedict['label_names']\n",
    "# We then reshape vectorized data into the image format\n",
    "X = X_train.reshape(50000, 3, 32, 32).transpose(0,2,3,1)\n",
    "\n",
    "print(category)\n",
    "print(X.shape)\n",
    "\n",
    "#Visualizing CIFAR 10 data. We randomly choose 25 images from the train dataset.\n",
    "fig, axes1 = plt.subplots(5,5,figsize=(8,8))\n",
    "for j in range(5):\n",
    "    for k in range(5):\n",
    "        i = np.random.choice(range(len(X)))\n",
    "        axes1[j][k].set_axis_off()\n",
    "        axes1[j][k].imshow(X[i:i+1][0])\n",
    "        axes1[j][k].set_title(category[y_train[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1559884658978,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "6YoejUt3HK_O",
    "outputId": "8f961d92-4960-4fb2-d496-5e0521996c7a"
   },
   "outputs": [],
   "source": [
    "# Data organization:\n",
    "#    Train data: 49,000 samples from the original train set: indices 1~49,000\n",
    "#    Validation data: 1,000 samples from the original train set: indices 49,000~50,000\n",
    "#    Test data: 1,000 samples from the original test set: indices 1~1,000\n",
    "#    Development data (for gradient check): 100 random samples from the train set: indices 1~49,000\n",
    "#    Development data (binary) (only for gradient check in Part 1): 100 random samples from the subsampled binary train set\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 100\n",
    "num_dev_binary = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "X_test = X_test[:num_test, :]\n",
    "y_test = y_test[:num_test]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-class dataset\n",
    "Next, in order to implement the experiment with the logistic regression classifier, we subsample the 10-class dataset to the 2-class dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample 10-class training set to 2-class training set\n",
    "X_train_binary = X_train[y_train<2,:]\n",
    "num_training_binary = X_train_binary.shape[0]\n",
    "y_train_binary = y_train[y_train<2]\n",
    "mask_binary = np.random.choice(num_training_binary, num_dev_binary, replace=False)\n",
    "\n",
    "X_val_binary = X_val[y_val<2,:]\n",
    "y_val_binary = y_val[y_val<2]\n",
    "\n",
    "X_dev_binary = X_train_binary[mask_binary]\n",
    "y_dev_binary = y_train_binary[mask_binary]\n",
    "\n",
    "\n",
    "print('Train data (binary) shape: ', X_train_binary.shape)\n",
    "print('Train labels (binary) shape: ', y_train_binary.shape)\n",
    "print('Validation data (binary) shape: ', X_val_binary.shape)\n",
    "print('Validation labels (binary) shape: ', y_val_binary.shape)\n",
    "print('Development data (binary) shape:', X_dev_binary.shape)\n",
    "print('Development data (binary) shape', y_dev_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2569,
     "status": "ok",
     "timestamp": 1559884662531,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "XWboKTBmHK_S",
    "outputId": "a81860fe-983b-44c7-8382-0b68b396815f"
   },
   "outputs": [],
   "source": [
    "# Preprocessing: subtract the mean value across every dimension, for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "\n",
    "# Append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: subtract the mean value across every dimension, for binary training data\n",
    "mean_image = np.mean(X_train_binary, axis=0)\n",
    "\n",
    "X_train_binary = X_train_binary.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val_binary = X_val_binary.astype(np.float32) - mean_image\n",
    "X_dev_binary = X_dev_binary.astype(np.float32) - mean_image\n",
    "\n",
    "# Append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train_binary = np.hstack([X_train_binary, np.ones((X_train_binary.shape[0], 1))])\n",
    "X_val_binary = np.hstack([X_val_binary, np.ones((X_val_binary.shape[0], 1))])\n",
    "X_dev_binary = np.hstack([X_dev_binary, np.ones((X_dev_binary.shape[0], 1))])\n",
    "print(X_train_binary.shape, X_val_binary.shape, X_dev_binary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_sQKxgGHK_V"
   },
   "source": [
    "## Part 1: Logistic Regression Classifier\n",
    "\n",
    "In this part, you are going to implement a logistic regression classifier. \n",
    "\n",
    "Let’s assume a training dataset of images $x_i \\in R^D$, each associated with a label $y_i$. Here $i=1 \\dots N$ and $y_i \\in 1 \\dots K$. That is, we have **N** examples (each with a dimensionality **D**) and **K** distinct categories.\n",
    "\n",
    "We will now define the score function $f: R^D \\to R^K$ that maps the raw image pixels to class scores: $$f(x_i; W, b)=W x_i + b$$\n",
    "where $W$ is of size $K \\times D$ and $b$ is of size $K \\times 1$. \n",
    "\n",
    "Here we will use **bias trick** to represent the two parameters $W,b$ as one by extending the vector $x_i$ with one additional dimension that always holds the constant **1** - a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply: $$f = f(x_i;W)=W x_i$$\n",
    "\n",
    "**Brief introduction to logistic regression classifier**\n",
    "\n",
    "Logistic regression classifier can solve a binary classification problem ($K=2$). A binary logistic regression classifier has only two classes (0,1), and calculates the probability of class 1 as:\n",
    "\n",
    "$$\n",
    "P(y=1 | x ; w)=\\frac{1}{1+e^{-f}}=\\sigma\\left(f\\right)\n",
    "$$\n",
    "\n",
    "Since the probabilities of class 1 and 0 sum to one, the probability for class 0 is:\n",
    "\n",
    "$$\n",
    "P(y=0 | x ; w)=1-P(y=1 | x ; w)\n",
    "$$\n",
    "\n",
    "Hence, an example is classified as a positive example ($y = 1$) if $\\sigma\\left(f\\right)>0.5$, or equivalently if the score $f>0$. The loss function then maximizes the log likelihood of this probability. You can convince yourself that this simplifies to:\n",
    "\n",
    "$$\n",
    "L_{i}=\\sum_{j} y_{i j} \\log \\left(\\sigma\\left(f_{j}\\right)\\right)+\\left(1-y_{i j}\\right) \\log \\left(1-\\sigma\\left(f_{j}\\right)\\right)\n",
    "$$\n",
    "\n",
    "where the labels $y_{ij}$ are assumed to be either 1 (positive) or 0 (negative), and $\\sigma(\\cdot)$ is the sigmoid function. The expression above can look scary but the gradient on $f$ is in fact extremely simple and intuitive: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{i}}{\\partial f_{j}}=y_{i j}-\\sigma\\left(f_{j}\\right)\n",
    "$$\n",
    "\n",
    "[1] http://cs231n.github.io/neural-networks-2/\n",
    "\n",
    "[2] https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_sQKxgGHK_V"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete the code in **./utils/classifiers/logistic_regression.py**. You have to implement logistic regression in two ways: \n",
    "naive and vectorized.\n",
    "We provide the verification code for you to check if your code works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1559884668395,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "v0DB0KWoHK_Y",
    "outputId": "eb47b0b6-6b34-4625-9a24-03dc219914fd"
   },
   "outputs": [],
   "source": [
    "# Verification code for checking the correctness of the implementation of logistic_regression\n",
    "\n",
    "from utils.classifiers.logistic_regression import logistic_regression_loss_naive\n",
    "from utils.classifiers.logistic_regression import logistic_regression_loss_vectorized\n",
    "\n",
    "# generate a random weight matrix seeded with small numbers\n",
    "np.random.seed(3456)\n",
    "W = np.random.randn(3073, 2) * 0.0001 \n",
    "\n",
    "## naive numpy implementation of Logistic Regression\n",
    "loss_naive, grad_naive = logistic_regression_loss_naive(W, X_dev_binary, y_dev_binary, 0.000005)\n",
    "print('naive numpy loss: {}.'.format(loss_naive))\n",
    "\n",
    "## vectorized numpy implementation of Logistic Regression\n",
    "loss_vec, grad_vec = logistic_regression_loss_vectorized(W, X_dev_binary, y_dev_binary, 0.000005)\n",
    "print('vectorized numpy loss: {}.'.format(loss_vec))\n",
    "\n",
    "## check the correctness\n",
    "print('*'*100)\n",
    "print('Relative loss error is {}'.format(abs(loss_vec-loss_naive)))\n",
    "grad_err = np.linalg.norm(grad_naive - grad_vec, ord='fro')\n",
    "print('Relative gradient error is {}'.format(grad_err))\n",
    "print('Is vectorized loss correct? {}'.format(np.allclose(loss_naive, loss_vec)))\n",
    "print('Is vectorized gradient correct? {}'.format(np.allclose(grad_naive, grad_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "7u9-MqILHK_b"
   },
   "source": [
    "## Part 2: Softmax classifier\n",
    "\n",
    "Softmax classifier is a generalization of the Logistic Regression classifier to multiple classes.\n",
    "\n",
    "In the Softmax classifier, the function mapping $f(x_i;W)=W x_i$ stays unchanged, but we now interpret the obtained scores as the unnormalized log probabilities for each class, and replace the hinge loss with a cross-entropy loss that has the form: $$L_i= - \\log (\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}).$$\n",
    "\n",
    "The cross-entropy between a “true” distribution $p$ and an estimated distribution $q$ is defined as: $$H(p, q)=- \\sum_x p(x) \\log q(x).$$\n",
    "\n",
    "Now, let's rewrite the expression of $L_i$: $$L_i= - \\sum_k p_{i,k} \\log (\\frac{e^{f_k}}{\\sum_j e^{f_j}})$$\n",
    "where $p_i=[0, \\dots,1, \\dots, 0]$ contains a single 1 at the $y_i$-th position, $p_{i,k}=p_i[k]$, $p_i \\in [1 \\times K]$.\n",
    "\n",
    "**Note:** Numerical stability. When you are writing code for computing the Softmax function in practice, the intermediate terms $e^{f_{y_i}}$ and $\\sum_j e^{f_j}$ may be very large due to the exponentials. Dividing with large numbers can be numerically unstable, so it is important to use the normalization trick. Notice that if we multiply both the top and the bottom of the fraction by constant $C$ and push $C$ inside the exponent, we get the following (mathematically equivalent) expression: $$\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}=\\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}}=\\frac{e^{f_{y_i}+\\log C}}{\\sum_j e^{f_j+\\log C}}.$$\n",
    "\n",
    "A common choice for $C$ is to set it to $\\log C= -\\max_j f_j$.\n",
    "\n",
    "In most cases, you also need to consider a bias term $b$ with length D. However, in this experiment, since a bias dimension has been added into the $X$, you can ignore it. \n",
    "\n",
    "**Softmax derivations (in matrix representation)**\n",
    "\n",
    "$$\\nabla_{W_k} L= - \\frac{1}{N} \\sum_i x_i^T(p_{i,m} - P_m) + 2 \\lambda W_k,$$\n",
    "where $P_k= \\frac{e^{f_k}}{\\sum_j e^{f_j}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "7u9-MqILHK_b"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete the code in **./utils/classifiers/softmax.py**.You have to implement the softmax layer in three ways. For the first two implementations, we provide the verification code for you to check if your implementation is correct. \n",
    "\n",
    "* Naive method using for-loop\n",
    "* Vectorized method\n",
    "* Softmax in Tensorflow. This step will familiarize you with TensorFlow functions. You can refer to the verification code.\n",
    "\n",
    "Do not forget the $L_2$ regularization term in the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1559884674707,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "DH7HdbEAHK_b",
    "outputId": "a6d3eb81-af5a-427e-f25e-4c811e36d847"
   },
   "outputs": [],
   "source": [
    "# Verification code for checking the correctness of the implementation of softmax implementations\n",
    "\n",
    "from utils.classifiers.softmax import softmax_loss_naive\n",
    "from utils.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "## generate a random weight matrix of small numbers\n",
    "np.random.seed(3456)\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "reg = tf.constant(0.000005)\n",
    "\n",
    "## ground truth of loss and gradient\n",
    "W_tf = tf.placeholder(tf.float32, shape=(3073,10))\n",
    "X = tf.placeholder(tf.float32, shape=(None, 3073))\n",
    "y = tf.placeholder(tf.int32, shape=(None,))\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits= tf.matmul(X, W_tf), labels=tf.one_hot(y,10))\n",
    "loss0 = tf.reduce_mean(cross_entropy) + reg*tf.reduce_sum(W_tf*W_tf)\n",
    "grad0 = tf.gradients(loss0, W_tf)\n",
    "out0 = (loss0, grad0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    tic = time.time()\n",
    "    loss_gt, grad_gt = sess.run(out0, feed_dict={W_tf: W, X: X_dev, y: y_dev})\n",
    "    toc = time.time()\n",
    "\n",
    "## naive softmax in numpy\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive numpy loss: {}, takes {} seconds.'.format(loss_naive, toc-tic))\n",
    "\n",
    "## vectorized softmax in numpy\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(loss_vec, toc-tic))\n",
    "\n",
    "## Verify your result here - use 'rel_err' for error evaluation.\n",
    "def rel_err(a,b):\n",
    "    return np.mean(abs(a-b))\n",
    "\n",
    "print('*'*100)\n",
    "print('Relative loss error of naive softmax is {}'.format(rel_err(loss_gt,loss_naive)))\n",
    "print('Relative loss error of vectorized softmax is {}'.format(rel_err(loss_gt,loss_vec)))\n",
    "print('Gradient error of naive softmax is {}'.format(rel_err(grad_gt,grad_naive)))\n",
    "print('Gradient error of vectorized softmax is {}'.format(rel_err(grad_gt,grad_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1e7981-XHK_f"
   },
   "source": [
    "## Part 3: Train your classifiers\n",
    "\n",
    "Now you can start to train your classifiers. We are going to use gradient descent algorithm for training, which differs from the usual logistic regression training process. \n",
    "\n",
    "<span style=\"color:red\"><strong>TODO</strong></span>: The original code is given in **./utils/classifiers/basic_classifier.py**. You need to complete functions **train** and **predict**, in the class **BasicClassifier**. Later, you use its subclasses **Logistic Regression** and **Softmax** to train the model seperately and verify your result.\n",
    "\n",
    "\n",
    "In the training section, you are asked to implement Stochastic gradient descent (SGD) optimization method. Pseudo code for SGD is shown below.\n",
    "\n",
    "* Stochastic gradient descent - SGD\n",
    "    ```\n",
    "    w = w - learning_rate * gradient \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H35ZjRpZHK_g"
   },
   "source": [
    "### Train Logistic Regression + Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11111,
     "status": "ok",
     "timestamp": 1559884688501,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "DjRy76pdHK_h",
    "outputId": "ca2707c6-3adf-4bc8-bbce-d30e2a0f0e59"
   },
   "outputs": [],
   "source": [
    "from utils.classifiers.basic_classifiers import Logistic_Regression\n",
    "\n",
    "## Logistic Regression + SGD\n",
    "classifier = Logistic_Regression()\n",
    "reg = 1e-5 # regularization\n",
    "lr = 1e-7 # learning rate\n",
    "loss_hist_sgd = classifier.train(X=X_train_binary, y=y_train_binary, learning_rate=lr, reg=reg, num_iters=1500, optim='SGD', verbose=True)\n",
    "\n",
    "# Write the BasicClassifier.predict function and evaluate the performance on both \n",
    "# training set and validation set\n",
    "y_train_pred = classifier.predict(X_train_binary)\n",
    "print('training accuracy: %f' % (np.mean(y_train_binary == y_train_pred), ))\n",
    "y_val_pred = classifier.predict(X_val_binary)\n",
    "print('validation accuracy: %f' % (np.mean(y_val_binary == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1559884692553,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "E_QGDSgvHK_k",
    "outputId": "70dd1a9c-3603-4faf-d8d2-625fbf050c20"
   },
   "outputs": [],
   "source": [
    "## SGD error plot\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HImk5vMHK_n"
   },
   "source": [
    "### Train Softmax + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11222,
     "status": "ok",
     "timestamp": 1559884706216,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "CR4ZDYF6HK_o",
    "outputId": "02212126-294d-4e77-c7d6-b23b94593ab2"
   },
   "outputs": [],
   "source": [
    "from utils.classifiers.basic_classifiers import Softmax\n",
    "\n",
    "## Softmax + SGD\n",
    "classifier = Softmax()\n",
    "reg = 1e-5 # regularization\n",
    "lr = 1e-7 # learning rate\n",
    "loss_hist_sgd = classifier.train(X=X_train, y=y_train, learning_rate=lr, reg=reg, num_iters=1500, optim='SGD', verbose=True)\n",
    "\n",
    "# Write the BasicClassifier.predict function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 574,
     "status": "ok",
     "timestamp": 1559884709159,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "SCl-qjtlHK_r",
    "outputId": "0d82c109-b1c9-43ea-90c5-6bc6084751af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## SGD loss curve\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task1-basic_classifiers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
