{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? \n",
    "\n",
    "   Your answer: **Softmax is used to predict value of categorical data which means each category has a probability under distribution. Thus, we can easily use loss to measure whether the prediction is accurate, because one category with higher probability will contribute to minimize the loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model.\n",
    "\n",
    "   Your answer: **In binary logistic regression, we use sigmoid function to get value. And then, we compare the value with threshold which is usually 0.5. Muli-class regression is a generalization of binary logistic regression.<br>** \n",
    "   <br>**Multi-class regression can be used to predict several classes (>=2 classes). Here we can use One-vs-all (one-vs-rest) classification to derive multi-class logistic regression from binary ones. We can pick every class at one time and set this class as positive and set the rest as negative. As we go through all the classes and do the same technique, we will get several classifier for each class.**<br>\n",
    "   <br>**In deep learning,we will use softmax function for output and calculate the probability. Always,the prediction with biggest prob will be regarded as the final output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: **Several reasons listed below: 1.Relu function has linear derivative which means it will not lost information in the extreme situation and avoids gradient vanishing in backpropogation. 2. Due to the simplicity of ReLu function, the workload of calculation will be much smaller. 3. Because many result will be zero, thus, the sparsity of matrix will also increase to avoid overfitting**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "\n",
    "   Your answer: **K-fold cross validation used to divide dataset into k folds, thus,we can select (k-1) data sets as training set and 1 data set as test set. The following is whole process: 1. Reshuffle all the data set. 2.Divide dataset into k folds. 3.Select (k-1) folds as training data set and 1 fold as test data set. Then fit the data into train set and use test set to calculate performance. 4. Summerize the model with best performance.**\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: **In the first place, I increase the hidden layer number as this hyper paramter will affect the performance most.Actually, 800 hidden layer numbers will contribute to the performance. Then, I increase the regularization term to 1e-3 to avoid overfitting. Moreover, I decrease the batch size for higher training accuracy. More epoches are taken in order to come down to smaller optimal value.**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: **[fill in here]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
