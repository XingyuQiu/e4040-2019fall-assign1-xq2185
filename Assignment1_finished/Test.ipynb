{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413, 0.98201379])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Logistics\n",
    "import numpy as np\n",
    "x = np.array([1,2,3,4])\n",
    "1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "def sigmoid(x):\n",
    "    h = np.zeros_like(x)\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO: Implement sigmoid function.                            #         \n",
    "    #############################################################################\n",
    "    #############################################################################\n",
    "    #                     START OF YOUR CODE                  #\n",
    "    #############################################################################\n",
    "    h = 1/(1+np.exp(-x))\n",
    "    \n",
    "    #############################################################################\n",
    "    #                     END OF YOUR CODE                   #\n",
    "    #############################################################################\n",
    "    return h \n",
    "\n",
    "def logistic_regression_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "      Logistic regression loss function, naive implementation (with loops)\n",
    "\n",
    "      Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "      of N examples.\n",
    "\n",
    "      Inputs:\n",
    "      - W: a numpy array of shape (D, C) containing weights.\n",
    "      - X: a numpy array of shape (N, D) containing a minibatch of data.\n",
    "      - y: a numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "        that X[i] has label c, where c can be either 0 or 1.\n",
    "      - reg: (float) regularization strength\n",
    "\n",
    "      Returns a tuple of:\n",
    "      - loss: (float) the mean value of loss functions over N examples in minibatch.\n",
    "      - gradient: gradient wrt W, an array of same shape as W\n",
    "    \"\"\"\n",
    "    # Set the loss to a random number\n",
    "    loss = None\n",
    "    # Initialize the gradient to zero\n",
    "    dW = None\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using explicit loops.    #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful    #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the      #\n",
    "    # regularization!                                        #\n",
    "    #############################################################################\n",
    "    #############################################################################\n",
    "    #                     START OF YOUR CODE                  #\n",
    "    #############################################################################\n",
    "    loss = 100000\n",
    "    dW = 0\n",
    "    step = 0.001\n",
    "    iteration = 5000\n",
    "    for iter_time in range(iteration):\n",
    "        m = X.shape[0]\n",
    "        sigma = sigmoid(X*W)\n",
    "        loss = -(y*np.log(sigma) + (1-y)*np.log(1-sigma))/m\n",
    "        dW = X.T*(y-sigma)/m\n",
    "        W = W - step*dW\n",
    "    return loss,dW\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    #                     END OF YOUR CODE                   #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "\n",
    "\n",
    "def logistic_regression_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Logistic regression loss function, vectorized version.\n",
    "\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Set the loss to a random number\n",
    "    loss = None\n",
    "    # Initialize the gradient to zero\n",
    "    dW = None\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: Compute the logistic regression loss and its gradient using no    # \n",
    "    # explicit loops.                                       #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful   #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the     #\n",
    "    # regularization!                                       #\n",
    "    ############################################################################\n",
    "    #############################################################################\n",
    "    #                     START OF YOUR CODE                  #\n",
    "    #############################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    #                     END OF YOUR CODE                   #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.54118485, 0.41067798],\n",
       "        [0.61109919, 0.47035551],\n",
       "        [0.19317701, 0.15881461],\n",
       "        [0.37073191, 0.29019106]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    h = 1/(1+np.exp(-x))\n",
    "    return h \n",
    "\n",
    "def logistic_regression_loss_naive(W, X, y):\n",
    "    loss = 100000\n",
    "    dW = 0\n",
    "    step = 0.001\n",
    "    iteration = 5000\n",
    "    for iter_time in range(iteration):\n",
    "        m = X.shape[0]\n",
    "        sigma = sigmoid(X*W)\n",
    "        #loss = -(np.dot(y,np.log(sigma)) + np.dot((1-y),np.log(1-sigma)))/m\n",
    "        dW = -X.T*(y-sigma)/m\n",
    "        W = W - step*dW\n",
    "    return dW\n",
    "W = np.matrix([[2,3],[5,6],[7,8]])\n",
    "X = np.random.rand(4,3)\n",
    "y = np.matrix([[1,0],[0,1],[1,0],[0,1]])\n",
    "W_1 = logistic_regression_loss_naive(W,X,y)\n",
    "X*W_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1]).reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "      Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "      Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "      of N examples.\n",
    "\n",
    "      Inputs:\n",
    "      - W: a numpy array of shape (D, C) containing weights.\n",
    "      - X: a numpy array of shape (N, D) containing a minibatch of data.\n",
    "      - y: a numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "        that X[i] has label c, where 0 <= c < C.\n",
    "      - reg: (float) regularization strength\n",
    "\n",
    "      Returns a tuple of:\n",
    "      - loss: (float) the mean value of loss functions over N examples in minibatch.\n",
    "      - gradient: gradient wrt W, an array of same shape as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using explicit loops.    #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful    #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the      #\n",
    "    # regularization!                                        #\n",
    "    #############################################################################\n",
    "    #############################################################################\n",
    "    #                     START OF YOUR CODE                  #\n",
    "    #############################################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    #############################################################################\n",
    "    #                     END OF YOUR CODE                   #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "\n",
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful    #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the      #\n",
    "    # regularization!                                        #\n",
    "    #############################################################################\n",
    "    #############################################################################\n",
    "    #                     START OF YOUR CODE                  #\n",
    "    #############################################################################\n",
    "    ##define gradient descent model\n",
    "    K = 10\n",
    "    def gradient_descent(W,X,y,class_num,iteration,step):\n",
    "        for iter_ in range(iteration):\n",
    "            value = np.exp(W*X)\n",
    "            rowsum = value.sum(axis=1)\n",
    "            rowsum = rowsum.repeat(class_num,axis=1)\n",
    "            loss = -y*np.log(value/rowsum)\n",
    "            dW = (y-value)X.shape[0]\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "    #############################################################################\n",
    "    #                     END OF YOUR CODE                   #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
